192/18

Voda REF: - Major incident.

Just over an hour for an update
FLT09028125
Rowan. 

=========================================

#TEMPEST LAB
lab deployment-overcloud-verify setup

- Install the Tempest testing service and component tests. 
    - - `sudo yum -y install openstack-tempest{,-all}`
- Create a test configuration directory, and populate it with configuration files using the configure-tempestdirectory script. 
    - - `mkdir tempest && cd tempest`
    - - `/usr/share/openstack-tempest<VERSION>/tools/configure-tempest-directory`
- Run the config-tempest script to configure the tests for the overcloud, using overcloudrc environment parameters, the external network ID and the cirros-0.3.4-x86_64-disk.img image from http://materials.example.com.
    - - `openstack network list`
    - - `tools/config_tempest.py --deployer-input ~/tempest-deployer-input.conf --debug --create identity.uri $OS_AUTH_URL identity.admin_password $OS_PASSWORD --image <path/to/image> --network-id <ID>`
- Configure and run a smoke test. The dynamic configuration in the previous step included mistral and designate component tests, which are not installed in this overcloud. Edit the configuration to disable mistral and designate testing. Use the test skip file found in student's Downloads directory on workstation to also exclude tests for API versions not in use on this overcloud. Exit from director after the test run.
    - - `vi ./etc/tempest.conf`
    - - `scp student@workstation:Downloads/tempest-smoke-skip-sample ./tempest-smoke-skip`
    - - `tempest cleanup --init-saved-state`
    - - `tools/run-tests.sh --skip-file ./tempest-smoke-skip --concurrency 1`
    - - `tempest cleanup`
=========================================

# KEYSTONE TOKENS / TRUNCATE
lab communication-svc-catalog setup

- On workstation, source the Keystone admin-rc file and list the Keystone endpoints registry. Take note of the available service names and types.
    - - `openstack endpoint list`
- View the Keystone service catalog and notice the endpoint URLs (especially the IP addresses), the version number, and the port number.
    - - `openstack catalog list`
    - - `openstack catalog show keystone`
- Issue an admin token to manually (using curl) find information about OpenStack.
    - - `openstack token issue`
- Verify the token retrieved in the previous command. Use the curl command with the token ID to retrieve the projects (tenants) for the admin user.
    - - `curl -H "X-Auth-Token: <token_ID>" <keystone_publicURL/tenants> | jq .`
 
-  Use SSH to connect to director as the user root. The database, MariaDB, resides on director and provides storage for expired tokens. Accessing MariaDB enables you to determine the amount of space used for expired tokens.
    - - `mysql -u root`
    - - `use keystone`
    - - `SELECT table_name, (data_length+index_length) tablesize FROM information_schema.tables;`
    - - `SELECT COUNT(*) FROM token WHERE token.expires < CONVERT_TZ(NOW(), @@session.time_zone, '+00:00');`
    - - `TRUNCATE TABLE token;`
    - - `SELECT COUNT(*) FROM token WHERE token.expires < CONVERT_TZ(NOW(), @@session.time_zone, '+00:00');`
    - - `exit`

- Ensure that the Keystone user has a cron job to flush tokens from the database.
    - - `crontab -u keystone -l`
- Modify the cron job to run keystone-manage token_flush hourly.
    - - `crontab -u keystone -e`

=========================================

# MESSAGE BROKER
lab communication-msg-brokering setup

- From workstation, use SSH to connect to director as the stack user. Use sudo to become the root user.
- Create a rabbitmq user named rabbitmqauth with redhat as the password.  
    - - `rabbitmqctl add_user <name> <password>`
- Configure permissions for the rabbitmqauth user. Use wildcard syntax to assign all resources to each of the three permissions for configure, write, and read.
    - - `rabbitmqctl set_permissions <name> ".*" ".*" ".*"`
- Set the administrator user tag to enable privileges for rabbitmqauth.
    - - `rabbitmqctl set_user_tags <name> administrator`
- Verify that a RabbitMQ Management configuration file exists in root's home directory. The contents should match as shown here.
    - - ``` 
        [default]
        hostname = 172.25.249.200
        port = 15672
        username = rabbitmqauth
        password = redhat
        ```
- Verify that rabbitmqauth is configured as an administrator.
    - - `rabbitmqctl list_users`
- Create an exchange topic named name.topic.
    - - `rabbitmqadmin -c .rabbitmqadmin.conf declare exchange name=name.topic type=topic`
- Verify that the exchange topic is created.
    - - `rabbitmqctl list_exchanges | grep name.topic`
- The next practice is to observe a message queue. Create a queue named redhat.queue
    - - `rabbitmqadmin -c .rabbitmqadmin.conf declare queue name=redhat.queue`
    - - `rabbitmqctl list_queues | grep redhat`
- Publish messages to the redhat.queue queue. These first two examples include the message payload on the command line.
    - - ```
        rabbitmqadmin -c rabbotmqadmin.conf publish routing_key=redhat.queue
        message1
        message2
        message3
        <ctrl+D>
        ```
- Verify the queue message count
    - - `rabbitmqctl list_queues | grep redhat`
- Display the first message in the queue. The message_count field indicates how many more messages exist after this one.
    - - `rabbitmqadmin -c .rabbitmqadmin.comf get queue=redhat.queue -f pretty_json`
- Display multiple messages using the count option. Each displayed message indicates how many more messages follow. The redelivered field indicates whether you have previously viewed this specific message.
    - - `rabbitmqadmin -c rabbitmqadmin.conf get queue=redhat.queue count=3 -f pretty_json`
- When finished, delete the queue named redhat.queue. Return to workstation
    - - `rabbitmqadmin -c .rabbitmqadmin.conf delete queue name=redhat.queue`
=========================================

#Troubleshoot Keystone and  RabbitMQ message broker.

lab communication-review setup

Scenario
    During a recent deployment of the overcloud, cloud administrators are reporting issues with the Compute and Image services. Cloud administrators are not able to access the Image service nor the Compute service APIs. You have been tasked with troubleshooting and fixing these issues.

Steps
1. From workstation, verify the issue by attempting to list instances as the OpenStack admin user. The command is expected to hang.
2. Use SSH to connect to controller0 as the heat-admin user to begin troubleshooting.
3. Check the Compute service logs for any applicable errors.
4. Investigate and fix the issue based on the error discovered in the log. Modify the incorrect rabbitmq port value in /etc/rabbitmq/rabbitmq-env.conf and use HUP signal to respawn the beam.smp process. Log out of the controller0 node when finished.
5. From workstation, attempt to aqgain list instances, to verify that the issue is fixed. This command is expected to display instances or return to a command prompt without hangiing.
6. Next, attempt to list images as well. The command is expected to fail, returning an internal server error.
7. Use SSH to connect to controller0 as the heat-admin user to begin troubleshooting.
8. Inspect the Image service logs for any applicable errors.
9. The error in the Image service log indicates a communication issue with the Image service API and the Identity service. In a previous step, you verified that the Identity service could communicate with the Compute service API, so the next logical step is to focus on the Image service configuration. Investigate and fix the issue based on the traceback found in the Image service log.
10. From workstation, again attempt to list images to verify the fix. This command should succeed and returning a command prompt without error.
Cleanup
=========================================

# Building and Customizing Images

lab customization-review setup

Resources
Base Image URL 
    http://materials.example.com/osp-small.qcow2
Diskimage-builder elements directory
    /usr/share/diskimage-builder/elements

Steps
1. From workstation, retrieve the osp-small.qcow2 image from http://materials.example.com/osp-small.qcow2 and save it in the /home/student/directory.
2. Create a copy of the diskimage-builder elements directory to work with in the /home/student/ directory.
3. Create a post-install.d directory under the working copy of the rhel7 element.
4. Add a script under the rhel7/post-install.d directory to enable the httpd service.
5. Export the following environment variables, which diskimage-builder requires.

Environment Variables
    export NODE_DIST=rhel7
    export DIB_LOCAL_IMAGE=/home/student/osp-small.qcow2
    export DIB_YUM_REPO_CONF="/etc/yum.repos.d/openstack.repo"
    export ELEMENTS_PATH=/home/student/elements

6. Build a RHEL 7 image named production-rhel-web.qcow2 using the diskimagebuilder elements configured previously. Include the httpd package in the image.
7. Add a custom web index page to the production-rhel-web.qcow2 image using guestfish. Include the text production-rhel-web in the index.html file. Ensure the  SELinux context of /var/www/html/index.html is correct.
8. As the operator1 user, create a new OpenStack image named production-rhel-web using the production-rhel-web.qcow2 image, with a minimum disk requirement of
10 GiB, and a minimum RAM requirement of 2 GiB.
9. As the operator1 user, launch an instance using the following attributes:
Instance Attributes 

flavor          m1.web
key pair        operator1-keypair1
network         production-network1
image           production-rhel-web
security group  production-web
name            production-web1
10. List the available floating IP addresses, and then allocate one to production-web1.
11. Log in to the production-web1 instance using operator1-keypair1.pem with ssh. Ensure the httpd package is installed, and that the httpd service is enabled and running.
12. From workstation, confirm that the custom web page, displayed from productionweb1, contains the text production-rhel-web.

Evaluation

lab customization-review grade
Cleanup
=========================================
# Configuring Ceph Storage

lab storage-config-ceph setup

Outcomes
You should be able to:
• Verify the status of a Ceph cluster.
• Verify Ceph pools and user for Red Hat OpenStack Platform services.
• Troubleshoot and fix an issue with a Ceph OSD.

1. Verify that the Ceph cluster status is HEALTH_OK.
2. Verify the status of the Ceph daemons and the cluster's latest events.
3. Verify that the pools and the openstack user, required for configuring Ceph as the back end for Red Hat OpenStack Platform services, are available.
4. Stop the OSD daemon with ID 0. Verify the Ceph cluster's status.
5. Start the OSD daemon with ID 0 to fix the issue. Verify that the Ceph cluster's status is HEALTH_OK.
=========================================

# Managing Object Storage / SWIFT

lab storage-review setup


Outcomes
You should be able to:
• Fix an issue in a Ceph environment.
• Upload a file to the Object storage service.
• Download and implement an object in the Object storage service inside an instance.

Steps
1. The Ceph cluster has a status issue. Fix the issue to return the status to HEALTH_OK.
2. As the operator1 user, create a new container called container4 in the Object storage service. Upload the custom MOTD file available at s to this container.
3. Log in to the production-web1 instance, and download the motd.custom object from
Swift to /etc/motd. Use the operator1 user credentials.
4. Verify that the MOTD file includes the message Updated MOTD message.
Evaluation
On workstation, run the lab storage-review grade command to confirm success of this
exercise.

=========================================


# Managing SDN Segments and Subnets

lab network-managing-sdn setup

Outcomes
You should be able to:
• Create networks
• Create routers
• Review the network implementation


Steps
1. From workstation, source the developer1-research-rc credentials file. As the developer1 user, create a network for the project. Name the network researchnetwork1.

2. Create the subnet research-subnet1 for the network in the 192.168.1.0/24 range. Use 172.25.250.254 as the DNS server.
    
3. Open another terminal and log in to the controller node, controller0, to review the ML2 configuration. Ensure that there are driver entries for VLAN networks.

4. On workstation, as the architect1 user, create the provider network provider-172.25.250. The network will be used to provide external connectivity. Use vlan as the provider network type with an segment ID of 500. Use datacentre as the physical network name, as defined in the ML2 configuration file.


5. Create the subnet for the provider network provider-172.25.250 with an allocation pool of 172.25.250.101 - 172.25.250.189. Name the subnet providersubnet-172.25.250. Use 172.25.250.254 for both the DNS server and the gateway. Disable DHCP for this network.

6. As the developer1 user, create the router research-router1. Add an interface to research-router1 in the research-subnet1 subnet. Define the router as a gateway for the provider-172.25.250 network.

7. Create a floating IP in the provider network, provider-172.25.250.
 
8. Launch the research-web1 instance in the environment. Use the m1.small flavor and the rhel7 image. Connect the instance to the research-network1 network.
 
9. Associate the floating IP, created previously, to the instance.
 
10. Open another terminal. Use the ssh command to log in to the compute0 virtual machine as the heat-admin user.

11. List the Linux bridges in the environment. Ensure that there is a qbr bridge that uses the first ten characters of the Neutron port in its name. The bridge has two ports in it: the TAP device that the instance uses and the qvb vEth pair, which connects the Linux bridge to the integration bridge.

12. Exit from the compute node and connect to the controller node.

13. To determine the port ID of the phy-br-ex bridge, use the ovs-ofctl command. The output lists the ports in the br-ex bridge.

14. Dump the flows for the external bridge, br-ex. Review the entries to locate the flow for the packets passing through the tenant network. Locate the rule that handles packets in the phy-br-ex port. The following output shows how the internal VLAN ID, 2, is replaced with the VLAN ID 500 as defined by the --provider-segment 500 option.
 
15. Exit from the controller0 node.

Cleanup
lab network-managing-sdn cleanup
=========================================

#Tracing Multitenancy Network Flows

lab network-tracing-net-flows setup

Outcomes
You should be able to:
• Create a router for multiple projects.
• Review the network implementation for multiple projects.
• Use Linux tools to trace network packets between multiple projects.

Steps
1. As the architect1 administrative user, review the instances for each of the two projects.
1.1. From workstation, source the credential file for the architect1 user in the
finance project, available at /home/student/architect1-finance-rc. List the
instances in the finance project.
[student@workstation ~]$ source architect1-finance-rc
[student@workstation ~(architect1-finance)]$ openstack server list -f json
[
{
"Status": "ACTIVE",
"Networks": "finance-network1=192.168.2.F",
"ID": "fcdd9115-5e05-4ec6-bd1c-991ab36881ee",
"Image Name": "rhel7",
"Name": "finance-app1"
}
]
1.2. Source the credential file of the architect1 user for the research project, available
at /home/student/architect1-research-rc. List the instances in the project.
[student@workstation ~(architect1-finance)]$ source architect1-research-rc
[student@workstation ~(architect1-research)]$ openstack server list -f json
[
{
"Status": "ACTIVE",
"Networks": "research-network1=192.168.1.R",
"ID": "d9c2010e-93c0-4dc7-91c2-94bce5133f9b",
"Image Name": "rhel7",
"Name": "research-app1"
}
]
2. As the architect1 administrative user in the research project, create a shared external network to provide external connectivity for the two projects. Use provider-172.25.250 as the name of the network. The environment uses flat networks with datacentre as the physical network name.

3. Create the subnet for the provider network in the 172.25.250.0/24 range. Name the subnet provider-subnet-172.25.250. Disable the DHCP service for the network and use an allocation pool of 172.25.250.101 - 172.25.250.189. Use 172.25.250.254 as the DNS server and the gateway for the network.

4. List the subnets present in the environment. Ensure that there are three subnets: one subnet for each project and one subnet for the external network.

5. Create the research-router1 router and connect it to the two subnets, finance and research.

6. Define the router as a gateway for the provider network, provider-172.25.250.

7. Ensure that the router is connected to the three networks by listing the router ports.

8. As the developer1 user, create a floating IP and attach it to the research-app1 virtual machine.

9. As the developer2 user, create a floating IP and attach it to the finance-app1 virtual machine.

10. Source the credentials for the developer1 user and retrieve the floating IP attached to the research-app1 virtual machine.

11. Test the connectivity to the instance research-app1, running in the research project by using the ping command.

12. As the developer2 user, retrieve the floating IP attached to the finance-app1 virtual machine so you can test connectivity.

13. Use the ping command to reach the 172.25.250.P IP. Leave the command running, as you will connect to the overcloud nodes to review how the packets are routed.

14. Open another terminal. Use the ssh command to log in to controller0 as the heatadmin user.

15. Run the tcpdump command against all interfaces. Notice the two IP address to which the ICMP packets are routed: 192.168.2.F, which is the private IP of the finance-app1 virtual machine, and 172.25.250.254, which is the gateway for the provider network.

16. Cancel the tcpdump command by pressing Ctrl+C and list the network namespaces. Retrieve the routes in the qrouter namespace to determine the network device that handles the routing for the 192.168.2.0/24 network. The following output indicates that packets destined to the 192.168.2.0/24 network are routed through the qr-ac11ea59-e5 device (the IDs and names will be different in your output).

17. Within the qrouter namespace, run the ping command to confirm that the private IP of the finance-app1 virtual machine, 192.168.2.F, is reachable.

18. From the first terminal, cancel the ping command by pressing Ctrl+C. Rerun the ping command against the floating IP of the research-app1 virtual machine, 172.25.250.N. Leave the command running, as you will be inspecting the packets from the controller0.

19. From the terminal connected to the controller-0, run the tcpdump command. Notice the two IP address to which the ICMP packets are routed: 192.168.1.R, which is the private IP of the research-app1 virtual machine, and 172.25.250.254, which is the IP address of the gateway for the provider network.

20. Cancel the tcpdump command by pressing Ctrl+C and list the network namespaces. Retrieve the routes in the qrouter namespace to determine the network device that handles routing for the 192.168.1.0/24 network. The following output indicates that packets destined to the 192.168.1.0/24 network are routed through the qr-fa7dab05- e5 device (the IDs and names will be different in your output).

21. Within the qrouter namespace, run the ping command to confirm that the private IP of the finance-app1 virtual machine, 192.168.1.F, is reachable.

22. Exit from controller0 and connect to compute0.

23. List the Linux bridges. The following output indicates two bridges with two ports each. Each bridge corresponds to an instance. The TAP devices in each bridge correspond to thevirtual NIC; the qvb devices correspond to the vEth pair that connect the Linux bridge to the integration bridge, br-int.

24. Run the tcpdump command against any of the two qvb interface while the ping command is still running against the 172.25.250.N floating IP. If the output does not show any packets being captured, press CTRL+C and rerun the command against the other qvb interface.

25. From the first terminal, cancel the ping command. Rerun the command against the 172.25.250.P IP, which is the IP of the finance-app1 instance.

26. From the terminal connected to compute0 node, enter CTRL+C to cancel the tcpdump command. Rerun the command against the second qvb interface, qvb03565cda-b1. Confirm that the output indicates some activity.

27. From the first terminal, cancel the ping and confirm that the IP address 192.168.2.F is the private IP of the finance-app1 instance.

28. Log in to the finance-app1 instance as the cloud-user user. Run the ping command against the floating IP assigned to the research-app1 virtual machine, 172.25.250.N.

29. From the terminal connected to compute-0, enter CTRL+C to cancel the tcpdump command. Rerun the command without specifying any interface. Confirm that the output indicates some activity.

30. Close the terminal connected to compute-0. Cancel the ping command, and log out of finance-app1.
Cleanup
lab network-tracing-net-flows cleanup


